{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model, BertTokenizer, BertForSequenceClassification\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Custom Dataset\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, tokenizer, word2vec_model):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.word2vec_model = word2vec_model\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        text = extract_text(audio_path)\n",
        "        word_vectors = self.get_word_vectors(text)\n",
        "        inputs = self.tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
        "            \"word_vectors\": torch.tensor(word_vectors, dtype=torch.float),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def get_word_vectors(self, text):\n",
        "        words = text.split()\n",
        "        vectors = [self.word2vec_model.wv[word] for word in words if word in self.word2vec_model.wv]\n",
        "        if len(vectors) == 0:\n",
        "            return np.zeros(self.word2vec_model.vector_size)\n",
        "        return np.mean(vectors, axis=0)\n",
        "\n",
        "# Function to extract BPM from audio file\n",
        "def extract_bpm(audio_path):\n",
        "    y, sr = librosa.load(audio_path, sr=None)\n",
        "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "    return tempo\n",
        "\n",
        "# Function to extract text from audio using pre-trained model\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "wav2vec2_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "def extract_text(audio_path):\n",
        "    try:\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "            outputs = wav2vec2_model(**inputs)\n",
        "            logits = outputs.last_hidden_state\n",
        "            transcription = processor.batch_decode(torch.argmax(logits, dim=-1))\n",
        "        return transcription[0] if transcription else \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        word_vectors = batch['word_vectors'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            word_vectors = batch['word_vectors'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            total_accuracy += (predictions == labels).sum().item()\n",
        "    return total_accuracy / len(dataloader.dataset)\n",
        "\n",
        "# Main analysis function\n",
        "def analyze_audio(audio_path):\n",
        "    bpm = extract_bpm(audio_path)\n",
        "    text = extract_text(audio_path)\n",
        "    return {\n",
        "        \"BPM\": bpm,\n",
        "        \"Transcription\": text\n",
        "    }\n",
        "\n",
        "# Load GTZAN dataset metadata and prepare file paths\n",
        "def load_gtzan_metadata(gtzan_dir):\n",
        "    genres = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]\n",
        "    file_paths = []\n",
        "    labels = []\n",
        "\n",
        "    # Navigate to 'genres_original' directory\n",
        "    genres_dir = os.path.join(gtzan_dir, \"genres_original\")\n",
        "\n",
        "    if not os.path.exists(genres_dir):\n",
        "        raise ValueError(f\"Could not find 'genres_original' directory in {gtzan_dir}. Please check the dataset structure.\")\n",
        "\n",
        "    for genre_idx, genre in enumerate(genres):\n",
        "        genre_dir = os.path.join(genres_dir, genre)\n",
        "        if os.path.exists(genre_dir):\n",
        "            genre_files = [file for file in os.listdir(genre_dir) if file.endswith(\".wav\")]\n",
        "            selected_files = genre_files[:10]  # Select only 10 files per genre\n",
        "            for file in selected_files:\n",
        "                file_paths.append(os.path.join(genre_dir, file))\n",
        "                labels.append(genre_idx)\n",
        "        else:\n",
        "            print(f\"Warning: Genre folder not found: {genre_dir}\")  # Print a warning if genre folder is missing\n",
        "\n",
        "    if not file_paths:  # Print error if no files were found.\n",
        "        raise ValueError(f\"No .wav files found in {gtzan_dir}. Please check the dataset path and structure.\")\n",
        "\n",
        "    return file_paths, labels, genres\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to GTZAN dataset directory\n",
        "    zip_path = \"/content/GTZAN.zip\"  # Update this path as needed\n",
        "    extract_dir = \"/content/GTZAN\"\n",
        "\n",
        "    # Extract zip file (if not already extracted)\n",
        "    if not os.path.exists(extract_dir):\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "    gtzan_dir = extract_dir\n",
        "\n",
        "    # Load metadata and file paths\n",
        "    file_paths, labels, genres = load_gtzan_metadata(gtzan_dir)\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "    # Train Word2Vec Model\n",
        "    texts = []\n",
        "    for audio in file_paths:  # Use selected files for Word2Vec training\n",
        "        text = extract_text(audio)\n",
        "        if text and text.strip():\n",
        "            texts.append(text)\n",
        "        else:\n",
        "            print(f\"Skipped empty transcription for {audio}\")\n",
        "    if not texts:\n",
        "        texts = [\"sample default text for word2vec\"]  # Fallback text for initialization\n",
        "        print(\"Fallback text used for Word2Vec initialization.\")\n",
        "\n",
        "    tokenized_texts = [text.split() for text in texts]\n",
        "    word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "    # Tokenizer and Model\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(genres))\n",
        "\n",
        "    # Dataset and Dataloader\n",
        "    train_files, val_files, train_labels, val_labels = train_test_split(file_paths, encoded_labels, test_size=0.2, random_state=42)\n",
        "    train_dataset = AudioDataset(train_files, train_labels, tokenizer, word2vec_model)\n",
        "    val_dataset = AudioDataset(val_files, val_labels, tokenizer, word2vec_model)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=4)\n",
        "\n",
        "    # Training setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 3\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
        "        val_accuracy = evaluate_model(model, val_loader, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "    # Analyze a sample audio file\n",
        "    sample_audio = \"/content/Making my tone.wav\"
        "    analysis_result = analyze_audio(sample_audio)\n",
        "    print(\"Analysis Result:\")\n",
        "    print(f\"BPM: {analysis_result['BPM']}\")\n",
        "    print(f\"Predicted Genre: {analysis_result['Genre']}\")\n",
        "    print(f\"Transcription: {analysis_result['Transcription']}\")\n"
      ],
      "metadata": {
        "id": "G4oYuDaifKXs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
