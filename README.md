Music Genre Classification Program

Motivation
저는 힙합 동아리에서 장기간 활동을 하며 취미로 힙합 음악을 만들고 있습니다. 음악을 오래 하다보니 다양한 장르의 음악에 도전을 해 볼 수 있었고 여러 장르의 음악을 섞으며 음악적 도전을 하였습니다. 그러던 과정에서 나의 음악이 과연 힙합일까라는 의문과 나의 음악을 어떠한 장르로 분류해야할지에 대한 의문을 가지게 되었습니다. 만약 음악을 수치화하여 이를 분석하여 어떠한 장르일지 구분을 할 수 있게되면 매우 편리하겠다는 생각을 가지게 되었고 이번 기계학습과 응용 수업에서 배운 내용들과 챗지피티의 도움으로 오디오 파일을 분석하여 장르를 구분해주는 코드를 프로젝트로 시도해보게 되었습니다.

개요
코드의 전체적인 흐름은 다음과 같습니다
- 100개의 언저리의 장르 정보가 들어있는 오디오 파일을 준비
- Wav2Vec을 사용해 오디오 데이터를 텍스트로 변환
- Librosa를 사용해 오디오의 BPM(Beats Per minute)을 추출
- 추출되고 변환된 데이터를 학습
- 데이터 셋으로 클래스를 구성하고 토큰화
- BERT를 사용하여 장르 구분
- 다시 학습
- 학습 완료 후에 나의 노래를 입력하고 bpm과 텍스트를 기반으로 음악 장르 분석

데이터 선택

음악적 데이터를 사용하기 위해서 많은 양의 오디오 데이터에 장르가 구분되어 있는 데이터가 필요했다. 그래서 Kaggle의 GTZAN을 사용하였습니다. GTZAN 데이터 세트는 오디오 장르 분류에 자주 사용되는 공개 데이터 세트로, 10개 장르에 대한 약 1000개의 30초짜리 오디오 트랙으로 구성되어 있습니다. 그 중에서 10개의 장르의 10개의 데이터를 사용하여 학습을 진행하였습니다.
https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification?resource=download-directory

데이터셋 처리 

PyTorch의 Dataset 클래스를 활용해 데이터를 처리하였습니다. AudioDataset 클래스를 통해 각 오디오 파일의 경로와 장르를 모았고 (장르는 레이블로) 입력될 데이터를 전처리하였습니다. 결국 이 클래스를 통해 오디오 파일 경로에서 데이터를 불러오고, 이를 텍스트로 변환한 뒤, 텍스트 데이터를 Word2Vec 임베딩으로 벡터화하여 학습 가능한 형식으로 바꾸어주었습니다. 각 데이터 항목은 input_ids(텍스트 토큰화 결과), attention_mask(패딩 정보), Word2Vec 임베딩 벡터, 그리고 장르 레이블로 구성되었고 이를 통해 텍스트와 오디오 데이터를 결합하여 장르 분류 모델의 입렵값으로 사용하였습니다.

BPM 추출 및 텍스트 변환 (오디오 데이터 처리)

오디오 파일에서 BPM과 텍스트를 추출하는 과정에서 두 가지 주요 함수를 사용했습니다. extract_bpm 함수는 librosa 라이브러리를 사용하여 오디오 신호에서 리듬 패턴을 분석하고, 음악의 속도를 측정합니다.
extract_text 함수는 Facebook의 사전 학습된 Wav2Vec2 모델을 사용하여 오디오 데이터를 텍스트로 변환합니다. 
이를 가지고 모델의 출력에서 텍스트를 디코딩하여 반환하였고 이 정보들을 장르 예측에 주요하게 사용하였습니다.

Word2Vec 임베딩을 통한 텍스트 벡터화 

여기서 수업 시간에 배운 워드 임베딩을 사용하였습니다. 추출된 텍스트를 Word2Vec 임베딩을 통해 벡터로 변환하였습니다. 텍스트 데이터를 문장 단위로 토큰화한 뒤, 단어별 임베딩 벡터를 계산하였고 텍스트 내 모든 단어의 벡터 평균을 취하여 고정된 크기의 입력 벡터를 생성하는 원리를 프로세스를 진행하게 코딩이 되었습니다. 이렇게 모델의 성능을 항상시킬 수 있었습니다.

데이터셋의 구성과 효율증진

GTZAN 데이터셋의 오디오 파일 경로와 레이블을 기반으로 학습과 검증 데이터를 분리하기 위해 train_test_split을 사용하였고 PyTorch의 DataLoader를 사용해 모델에 전달됩니다. DataLoader는 데이터를 효율적으로 로드하고 배치를 구성하여 모델 학습 속도를 높이는 용도로 사용되었습니다.

모델 구성 및 학습 프로세스 

장르 분류 모델은 사전 학습된 BERT 모델(bert-base-uncased)을 기반으로 구축되었습니다. 이 모델은 텍스트 데이터를 입력받아 장르를 분류합니다. 입력 데이터(input_ids, attention_mask, Word2Vec 임베딩 벡터)를 모델에 전달하고 손실 함수(CrossEntropyLoss)를 사용하여 모델 예측과 실제 레이블 간의 차이를 계산한 뒤 옵티마이저(Adam)를 통해 손실을 줄이도록 모델의 가중치를 업데이트하는 과정으로 학습이 진행됩니다. 
학습 데이터 전체를 반복(Epoch)하며, 검증 데이터를 사용해 모델 성능을 평가합니다. 그리고 BERT 토크나이저와 Word2Vec 벡터를 활용하여 Torch 데이터셋으로 변환이 된 것입니다.

새로운 오디오 파일 장르 구분

새로 입력된 오디오 파일에서 BPM과 텍스트를 추출하고 변환된 텍스트를 워드임베딩으로 벡터화후에 학습된 모델에 데이터를 입력하여 장르를 예측하여 장르를 구분해주는 형태로 코드가 마무리 됩니다. 이 분석 결과는 BPM, 텍스트, 예측된 장르로 출력됩니다.


후기

먼저 음악을 코딩과 머신러닝으로 결합할 수 있어서 매우 즐겁게 진행한 프로젝트였습니다. 그러나 하면서 제가 알지 못하는 다양한 함수나 기능들이 필요했고 지피티와 많은 시간을 대화하며 하나씩 코드들을 채워가고 풀어가는 재미가 있었습니다. 저 코드에 나와있는 Making my tone은 저의 음악으로 실제로 실험을 해본 결과 힙합 장르라고 뜨는 것에 놀라웠습니다. BPM도 맞았으나 가사를 가져오는 것에 있어서는 많은 오류가 있기는 하였습니다. 저의 다른 곡들을 10개  정도를 시도해본 결과 metal과 pop이 나오는 결과도 받을 수 있었고 country와 같은 오류의 결과가 3번이나 나오기도 하였습니다.그리고 가사를 가져오는 부분에서 아직 정확하지 못한 부분이 많았습니다.
이 프로젝트를 진행하면서 단점으로는 떨어지는 정확성이 있었고 여기에 더불어 코드가 학습을 하는 것에 많은 시간이 걸린다는 부분이 있었습니다. 거의 30분에서 45분이 걸리며 결과를 가져왔습니다. 이러한 부분들을 개선하기 위해서 다음에는 사소한 부분들에서 학습이 필요 없는 pretrained 모델을 보다 더 적극적으로 사용해보고 싶기도 하였으며 데이터를 조금 더 전처리를 깔끔하게 하고 파인 튜닝을 좀 더 세부적으로 할 수 있었으면 더 좋은 결과를 얻을 수 있었을 것 같습니다.

GPT를 활용하여 이 프로젝트를 요약을 해보면 다음과 같이 설명합니다

프로젝트 요약: 오디오 데이터를 기반으로 한 BPM 추출, 텍스트 전사 및 장르 분류

1. 프로젝트 목표
* 오디오 데이터를 활용하여 다음의 세 가지 작업을 수행하는 것을 목표로 함:
    1. 오디오 파일에서 BPM(비트 당 박자) 추출
    2. 오디오를 텍스트로 전사(Transcription) 처리
    3. 전사된 텍스트 데이터를 바탕으로 장르를 분류

2. 주요 기술 및 도구
1. Wav2Vec2:
    * 음성 데이터를 텍스트로 변환하는 사전 학습된 모델.
    * 오디오 파일의 음성 데이터를 로드하고 이를 텍스트로 전사하는 데 사용됨.
2. BERT:
    * 입력된 텍스트를 기반으로 장르 분류를 수행하는 데 사용.
    * BertForSequenceClassification 클래스를 사용해 사전 학습된 모델을 미세 조정하여 장르 분류를 학습함.
3. Librosa:
    * 오디오 데이터에서 **BPM(Tempo)**을 추출.
    * 오디오 파일을 로드하고, 박자 관련 정보를 계산.
4. GTZAN Dataset:
    * 장르별로 분류된 오디오 데이터를 사용하여 학습 및 테스트.
    * 장르: blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, rock.
5. Word2Vec:
    * 전사된 텍스트에서 단어 벡터를 생성.
    * 추가적인 입력 피처로 활용하여 장르 분류 모델의 성능을 보완.
6. PyTorch:
    * 데이터셋 처리, 모델 학습, 평가를 위한 주요 프레임워크.

3. 프로젝트 주요 단계
1. 데이터 준비 및 전처리
    * GTZAN 데이터셋의 경로 설정 및 압축 해제.
    * 각 장르별로 .wav 파일 10개씩 추출하여 학습 및 검증에 사용.
    * 파일 경로와 레이블(Label)을 저장.
2. 오디오 분석
    * BPM 추출:
        * Librosa를 사용해 오디오의 박자 정보를 계산.
    * 텍스트 전사:
        * Wav2Vec2 모델을 사용해 오디오에서 텍스트를 추출.
        * 텍스트가 비어 있거나 오류가 발생하는 경우 예외 처리를 수행.
3. 장르 분류 모델 준비
    * Word2Vec:
        * 전사된 텍스트 데이터를 기반으로 단어 임베딩을 생성.
    * BERT:
        * 텍스트 데이터를 입력받아 각 장르에 대한 확률 분포를 계산.
        * 최종적으로 가장 높은 확률의 장르를 예측.
4. 데이터셋 구성
    * AudioDataset 클래스:
        * 오디오 파일 경로, 텍스트, Word2Vec 임베딩, BERT 입력값을 포함한 커스텀 데이터셋 생성.
    * 학습(train)과 검증(validation) 데이터셋으로 분리.
5. 모델 학습 및 평가
    * 학습:
        * train_model 함수에서 PyTorch를 사용해 BERT 모델 학습.
        * CrossEntropyLoss와 Adam Optimizer를 사용.
    * 평가:
        * evaluate_model 함수에서 검증 데이터셋으로 모델의 정확도를 측정.
6. 오디오 파일 분석
    * analyze_audio 함수:
        * 입력된 오디오 파일에서 BPM, 전사된 텍스트, 예측된 장르를 반환.
    * 최종적으로 BPM, 텍스트 전사, 예측된 장르를 출력.

4. 주요 코드 흐름
* GTZAN 데이터셋 경로 설정 → 데이터 압축 해제 → 오디오 파일과 레이블 로드
* Wav2Vec2로 텍스트 추출 → 텍스트를 Word2Vec와 BERT 입력으로 변환
* BERT 모델 학습 → 학습된 모델로 새로운 오디오 파일 분석 및 장르 예측

5. 출력 예시
1. BPM:
    * 입력된 오디오 파일의 비트 당 박자를 출력.
    * 예: "BPM: 120"
2. Transcription:
    * 오디오에서 추출한 텍스트(예: 노래 가사, 음성) 출력.
    * 예: "Transcription: Hello world, this is a song."
3. Predicted Genre:
    * 학습된 모델로 예측한 장르를 출력.
    * 예: "Predicted Genre: pop"

6. 프로젝트의 의의
* 음성 데이터 분석: Wav2Vec2를 사용해 음성 데이터를 텍스트로 변환함으로써 NLP 모델과의 통합 가능성을 확인.
* 텍스트 기반 장르 분류: Word2Vec와 BERT를 활용해 오디오 데이터를 텍스트로 전환한 후, 장르를 정확히 분류.
* 음성+음악 데이터를 결합한 분석: BPM 및 텍스트를 결합해 음악 분석의 확장 가능성을 보여줌.

이 프로젝트는 오디오 데이터에서 추출된 다중 정보를 사용해 음악 장르를 분류하며, 음악 데이터 분석 및 딥러닝의 실제 적용 사례를 탐구하는 데 유용합니다.









